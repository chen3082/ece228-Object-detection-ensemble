{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f71821eb",
      "metadata": {
        "id": "f71821eb"
      },
      "outputs": [],
      "source": [
        "#!unzip PennFudanPed.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2cd93dd",
      "metadata": {
        "id": "f2cd93dd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from PIL import Image\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import math\n",
        "          \n",
        "class PennFudanDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms=None):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images ad masks\n",
        "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
        "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        # note that we haven't converted the mask to RGB,\n",
        "        # because each color corresponds to a different instance\n",
        "        # with 0 being background\n",
        "        mask = Image.open(mask_path)\n",
        "\n",
        "        mask = np.array(mask)\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = np.unique(mask)\n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:]\n",
        "\n",
        "        # split the color-encoded mask into a set\n",
        "        # of binary masks\n",
        "        masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        num_objs = len(obj_ids)\n",
        "        boxes = []\n",
        "        for i in range(num_objs):\n",
        "            pos = np.where(masks[i])\n",
        "            xmin = np.min(pos[1])\n",
        "            xmax = np.max(pos[1])\n",
        "            ymin = np.min(pos[0])\n",
        "            ymax = np.max(pos[0])\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        # there is only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            #img, target = self.transforms(img, target)\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "dataset = PennFudanDataset('PennFudanPed/')\n",
        "\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import RetinaNet\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import torchvision.transforms as T\n",
        "\n",
        "\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    # converts the image, a PIL image, into a PyTorch Tensor\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "        # during training, randomly flip the training images\n",
        "        # and ground-truth for data augmentation\n",
        "        #transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "        transforms.append(T.ColorJitter(hue=(-0.4, 0.4)))\n",
        "\n",
        "    return T.Compose(transforms)\n",
        "\n",
        "# use our dataset and defined transformations\n",
        "dataset = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
        "dataset111 = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
        "\n",
        "dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
        "\n",
        "# split the dataset in train and test set\n",
        "# torch.manual_seed(1)\n",
        "# indices = torch.randperm(len(dataset)).tolist()\n",
        "# dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
        "# dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
        "\n",
        "# split the dataset in train and test set\n",
        "torch.manual_seed(1)\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "#each supgroup should have 90 pics\n",
        "dataset5 = torch.utils.data.Subset(dataset, indices[:120])\n",
        "\n",
        "dataset1 = torch.utils.data.Subset(dataset, indices[:90])\n",
        "dataset2 = torch.utils.data.Subset(dataset, indices[:60]+indices[90:120])\n",
        "dataset3 = torch.utils.data.Subset(dataset, indices[:30]+indices[60:120])\n",
        "dataset4 = torch.utils.data.Subset(dataset, indices[30:120])\n",
        "\n",
        "dataset1111 = torch.utils.data.Subset(dataset111, indices[:120])\n",
        "\n",
        "dataset11 = torch.utils.data.Subset(dataset111, indices[:90])\n",
        "dataset21 = torch.utils.data.Subset(dataset111, indices[:60]+indices[90:120])\n",
        "dataset31 = torch.utils.data.Subset(dataset111, indices[:30]+indices[60:120])\n",
        "dataset41 = torch.utils.data.Subset(dataset111, indices[30:120])\n",
        "\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
        "\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset5, batch_size=2, shuffle=True, pin_memory=True,\n",
        "    collate_fn=utils.collate_fn)\n",
        "data_loader22 = torch.utils.data.DataLoader(\n",
        "    dataset1111, batch_size=2, shuffle=True, pin_memory=True,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader1 = torch.utils.data.DataLoader(\n",
        "    dataset1, batch_size=2, shuffle=True, pin_memory=True,\n",
        "    collate_fn=utils.collate_fn)\n",
        "data_loader2 = torch.utils.data.DataLoader(\n",
        "    dataset2, batch_size=2, shuffle=True, pin_memory=True,\n",
        "    collate_fn=utils.collate_fn)\n",
        "data_loader3 = torch.utils.data.DataLoader(\n",
        "    dataset3, batch_size=2, shuffle=True, pin_memory=True,\n",
        "    collate_fn=utils.collate_fn)\n",
        "data_loader4 = torch.utils.data.DataLoader(\n",
        "    dataset4, batch_size=2, shuffle=True, pin_memory=True,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader11 = torch.utils.data.DataLoader(\n",
        "    dataset11, batch_size=2, shuffle=True, pin_memory=True,\n",
        "    collate_fn=utils.collate_fn)\n",
        "data_loader21 = torch.utils.data.DataLoader(\n",
        "    dataset21, batch_size=2, shuffle=True, pin_memory=True,\n",
        "    collate_fn=utils.collate_fn)\n",
        "data_loader31 = torch.utils.data.DataLoader(\n",
        "    dataset31, batch_size=2, shuffle=True, pin_memory=True,\n",
        "    collate_fn=utils.collate_fn)\n",
        "data_loader41 = torch.utils.data.DataLoader(\n",
        "    dataset41, batch_size=2, shuffle=True, pin_memory=True,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=1, shuffle=False, pin_memory=True,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# our dataset has two classes only - background and person\n",
        "num_classes = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ba55ebb",
      "metadata": {
        "id": "6ba55ebb"
      },
      "outputs": [],
      "source": [
        "# # load models\n",
        "# model_retina_res = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=False)\n",
        "# model_retina_res.load_state_dict(torch.load('models/retina_res_aug.pth'))\n",
        "\n",
        "# model_faster_res = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
        "# model_faster_res.load_state_dict(torch.load('models/faster_res_aug.pth'))\n",
        "\n",
        "# model_faster_mo = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=False)\n",
        "# model_faster_mo.load_state_dict(torch.load('models/faster_mo_aug.pth'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80fc65d7",
      "metadata": {
        "id": "80fc65d7"
      },
      "source": [
        "### retinanet with resnet50 backbone submodel, if want to perform data augmentation, uncomment last five train_one_epoch function¶"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69d5b77d",
      "metadata": {
        "id": "69d5b77d",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "\n",
        "model_retina_res1 = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True)\n",
        "model_retina_res2 = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True)\n",
        "model_retina_res3 = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True)\n",
        "model_retina_res4 = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True)\n",
        "\n",
        "# move model to the right device\n",
        "model_retina_res1.to(device)\n",
        "model_retina_res2.to(device)\n",
        "model_retina_res3.to(device)\n",
        "model_retina_res4.to(device)\n",
        "\n",
        "# construct an optimizer\n",
        "params1 = [p1 for p1 in model_retina_res1.parameters() if p1.requires_grad]\n",
        "params2 = [p2 for p2 in model_retina_res2.parameters() if p2.requires_grad]\n",
        "params3 = [p3 for p3 in model_retina_res3.parameters() if p3.requires_grad]\n",
        "params4 = [p4 for p4 in model_retina_res4.parameters() if p4.requires_grad]\n",
        "\n",
        "optimizer1 = torch.optim.SGD(params1, lr=0.005,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "optimizer2 = torch.optim.SGD(params2, lr=0.005,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "optimizer3 = torch.optim.SGD(params3, lr=0.005,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "optimizer4 = torch.optim.SGD(params4, lr=0.005,\n",
        "                           momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# and a learning rate scheduler which decreases the learning rate by\n",
        "# 10x every 3 epochs\n",
        "lr_scheduler1 = torch.optim.lr_scheduler.StepLR(optimizer1,\n",
        "                                               step_size=3,\n",
        "                                               gamma=0.1)\n",
        "lr_scheduler2 = torch.optim.lr_scheduler.StepLR(optimizer2,\n",
        "                                               step_size=3,\n",
        "                                               gamma=0.1)\n",
        "lr_scheduler3 = torch.optim.lr_scheduler.StepLR(optimizer3,\n",
        "                                               step_size=3,\n",
        "                                               gamma=0.1)\n",
        "lr_scheduler4 = torch.optim.lr_scheduler.StepLR(optimizer4,\n",
        "                                               step_size=3,\n",
        "                                               gamma=0.1)\n",
        "\n",
        "for epoch in range(10):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_one_epoch(model_retina_res1, optimizer1, data_loader1, device, epoch, print_freq=10)\n",
        "    train_one_epoch(model_retina_res2, optimizer2, data_loader2, device, epoch, print_freq=10)\n",
        "    train_one_epoch(model_retina_res3, optimizer3, data_loader3, device, epoch, print_freq=10)\n",
        "    train_one_epoch(model_retina_res4, optimizer4, data_loader4, device, epoch, print_freq=10)\n",
        "#uncomment here to do data augmentation\n",
        "#     train_one_epoch(model_retina_res1, optimizer1, data_loader11, device, epoch, print_freq=10)\n",
        "#     train_one_epoch(model_retina_res2, optimizer2, data_loader21, device, epoch, print_freq=10)\n",
        "#     train_one_epoch(model_retina_res3, optimizer3, data_loader31, device, epoch, print_freq=10)\n",
        "#     train_one_epoch(model_retina_res4, optimizer4, data_loader41, device, epoch, print_freq=10)\n",
        "\n",
        "    \n",
        "    # update the learning rate\n",
        "    lr_scheduler1.step()\n",
        "    lr_scheduler2.step()\n",
        "    lr_scheduler3.step()\n",
        "    lr_scheduler4.step()\n",
        "\n",
        "    # evaluate on the test dataset\n",
        "evaluate(model_retina_res1, data_loader_test, device=device)\n",
        "evaluate(model_retina_res2, data_loader_test, device=device)\n",
        "evaluate(model_retina_res3, data_loader_test, device=device)\n",
        "evaluate(model_retina_res4, data_loader_test, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6fa9024",
      "metadata": {
        "id": "a6fa9024"
      },
      "source": [
        "### FasterRCNN with resnet50 backbone submodel,if want to perform data augmentation,uncomment last five train_one_epoch function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be51f3d9",
      "metadata": {
        "id": "be51f3d9"
      },
      "outputs": [],
      "source": [
        "\n",
        "model_retina_res1 = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model_retina_res2 = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model_retina_res3 = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model_retina_res4 = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "# move model to the right device\n",
        "model_retina_res1.to(device)\n",
        "model_retina_res2.to(device)\n",
        "model_retina_res3.to(device)\n",
        "model_retina_res4.to(device)\n",
        "\n",
        "# construct an optimizer\n",
        "params1 = [p1 for p1 in model_retina_res1.parameters() if p1.requires_grad]\n",
        "params2 = [p2 for p2 in model_retina_res2.parameters() if p2.requires_grad]\n",
        "params3 = [p3 for p3 in model_retina_res3.parameters() if p3.requires_grad]\n",
        "params4 = [p4 for p4 in model_retina_res4.parameters() if p4.requires_grad]\n",
        "\n",
        "optimizer1 = torch.optim.SGD(params1, lr=0.005,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "optimizer2 = torch.optim.SGD(params2, lr=0.005,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "optimizer3 = torch.optim.SGD(params3, lr=0.005,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "optimizer4 = torch.optim.SGD(params4, lr=0.005,\n",
        "                           momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# and a learning rate scheduler which decreases the learning rate by\n",
        "# 10x every 3 epochs\n",
        "lr_scheduler1 = torch.optim.lr_scheduler.StepLR(optimizer1,\n",
        "                                               step_size=3,\n",
        "                                               gamma=0.1)\n",
        "lr_scheduler2 = torch.optim.lr_scheduler.StepLR(optimizer2,\n",
        "                                               step_size=3,\n",
        "                                               gamma=0.1)\n",
        "lr_scheduler3 = torch.optim.lr_scheduler.StepLR(optimizer3,\n",
        "                                               step_size=3,\n",
        "                                               gamma=0.1)\n",
        "lr_scheduler4 = torch.optim.lr_scheduler.StepLR(optimizer4,\n",
        "                                               step_size=3,\n",
        "                                               gamma=0.1)\n",
        "\n",
        "for epoch in range(10):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_one_epoch(model_retina_res1, optimizer1, data_loader1, device, epoch, print_freq=10)\n",
        "    train_one_epoch(model_retina_res2, optimizer2, data_loader2, device, epoch, print_freq=10)\n",
        "    train_one_epoch(model_retina_res3, optimizer3, data_loader3, device, epoch, print_freq=10)\n",
        "    train_one_epoch(model_retina_res4, optimizer4, data_loader4, device, epoch, print_freq=10)\n",
        "\n",
        "#     train_one_epoch(model_retina_res1, optimizer1, data_loader11, device, epoch, print_freq=10)\n",
        "#     train_one_epoch(model_retina_res2, optimizer2, data_loader21, device, epoch, print_freq=10)\n",
        "#     train_one_epoch(model_retina_res3, optimizer3, data_loader31, device, epoch, print_freq=10)\n",
        "#     train_one_epoch(model_retina_res4, optimizer4, data_loader41, device, epoch, print_freq=10)\n",
        "\n",
        "    \n",
        "    # update the learning rate\n",
        "    lr_scheduler1.step()\n",
        "    lr_scheduler2.step()\n",
        "    lr_scheduler3.step()\n",
        "    lr_scheduler4.step()\n",
        "\n",
        "    # evaluate on the test dataset\n",
        "evaluate(model_retina_res1, data_loader_test, device=device)\n",
        "evaluate(model_retina_res2, data_loader_test, device=device)\n",
        "evaluate(model_retina_res3, data_loader_test, device=device)\n",
        "evaluate(model_retina_res4, data_loader_test, device=device)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4176a62",
      "metadata": {
        "id": "e4176a62"
      },
      "source": [
        "### FasterRCNN with mobileNet backbone submodel,if want to perform data augmentation, uncomment last five train_one_epoch function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cb7fc18",
      "metadata": {
        "id": "9cb7fc18"
      },
      "outputs": [],
      "source": [
        "\n",
        "model_retina_res1 = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
        "model_retina_res2 = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
        "model_retina_res3 = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
        "model_retina_res4 = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
        "\n",
        "# move model to the right device\n",
        "model_retina_res1.to(device)\n",
        "model_retina_res2.to(device)\n",
        "model_retina_res3.to(device)\n",
        "model_retina_res4.to(device)\n",
        "\n",
        "# construct an optimizer\n",
        "params1 = [p1 for p1 in model_retina_res1.parameters() if p1.requires_grad]\n",
        "params2 = [p2 for p2 in model_retina_res2.parameters() if p2.requires_grad]\n",
        "params3 = [p3 for p3 in model_retina_res3.parameters() if p3.requires_grad]\n",
        "params4 = [p4 for p4 in model_retina_res4.parameters() if p4.requires_grad]\n",
        "\n",
        "optimizer1 = torch.optim.SGD(params1, lr=0.005,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "optimizer2 = torch.optim.SGD(params2, lr=0.005,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "optimizer3 = torch.optim.SGD(params3, lr=0.005,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "optimizer4 = torch.optim.SGD(params4, lr=0.005,\n",
        "                           momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# and a learning rate scheduler which decreases the learning rate by\n",
        "# 10x every 3 epochs\n",
        "lr_scheduler1 = torch.optim.lr_scheduler.StepLR(optimizer1,\n",
        "                                               step_size=3,\n",
        "                                               gamma=0.1)\n",
        "lr_scheduler2 = torch.optim.lr_scheduler.StepLR(optimizer2,\n",
        "                                               step_size=3,\n",
        "                                               gamma=0.1)\n",
        "lr_scheduler3 = torch.optim.lr_scheduler.StepLR(optimizer3,\n",
        "                                               step_size=3,\n",
        "                                               gamma=0.1)\n",
        "lr_scheduler4 = torch.optim.lr_scheduler.StepLR(optimizer4,\n",
        "                                               step_size=3,\n",
        "                                               gamma=0.1)\n",
        "\n",
        "for epoch in range(10):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_one_epoch(model_retina_res1, optimizer1, data_loader1, device, epoch, print_freq=10)\n",
        "    train_one_epoch(model_retina_res2, optimizer2, data_loader2, device, epoch, print_freq=10)\n",
        "    train_one_epoch(model_retina_res3, optimizer3, data_loader3, device, epoch, print_freq=10)\n",
        "    train_one_epoch(model_retina_res4, optimizer4, data_loader4, device, epoch, print_freq=10)\n",
        "\n",
        "#     train_one_epoch(model_retina_res1, optimizer1, data_loader11, device, epoch, print_freq=10)\n",
        "#     train_one_epoch(model_retina_res2, optimizer2, data_loader21, device, epoch, print_freq=10)\n",
        "#     train_one_epoch(model_retina_res3, optimizer3, data_loader31, device, epoch, print_freq=10)\n",
        "#     train_one_epoch(model_retina_res4, optimizer4, data_loader41, device, epoch, print_freq=10)\n",
        "\n",
        "    \n",
        "    # update the learning rate\n",
        "    lr_scheduler1.step()\n",
        "    lr_scheduler2.step()\n",
        "    lr_scheduler3.step()\n",
        "    lr_scheduler4.step()\n",
        "\n",
        "    # evaluate on the test dataset\n",
        "evaluate(model_retina_res1, data_loader_test, device=device)\n",
        "evaluate(model_retina_res2, data_loader_test, device=device)\n",
        "evaluate(model_retina_res3, data_loader_test, device=device)\n",
        "evaluate(model_retina_res4, data_loader_test, device=device)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4786bdcb",
      "metadata": {
        "id": "4786bdcb"
      },
      "source": [
        "Adjust IoU here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acbb17e5",
      "metadata": {
        "id": "acbb17e5"
      },
      "outputs": [],
      "source": [
        "def nonMaximumSuppression(boxes, overlapThresh):\n",
        "    # if there are no boxes, return an empty list\n",
        "\n",
        "    # initialize the list of picked indexes\n",
        "    pick = []\n",
        "    probFinal = 0\n",
        "    # grab the coordinates of the bounding boxes\n",
        "    x1 = boxes[:, 0].astype(float)\n",
        "    y1 = boxes[:, 1].astype(float)\n",
        "    x2 = boxes[:, 2].astype(float)\n",
        "    y2 = boxes[:, 3].astype(float)\n",
        "\n",
        "    # compute the area of the bounding boxes and sort the bounding\n",
        "    # boxes by the bottom-right y-coordinate of the bounding box\n",
        "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
        "    idxs = np.argsort(y2)\n",
        "    # keep looping while some indexes still remain in the indexes\n",
        "    # list\n",
        "    while len(idxs) > 0:\n",
        "        # grab the last index in the indexes list, add the index\n",
        "        # value to the list of picked indexes, then initialize\n",
        "        # the suppression list (i.e. indexes that will be deleted)\n",
        "        # using the last index\n",
        "        last = len(idxs) - 1\n",
        "        i = idxs[last]\n",
        "        pick.append(i)\n",
        "        suppress = [last]\n",
        "        # loop over all indexes in the indexes list\n",
        "        for pos in range(0, last):\n",
        "            # grab the current index\n",
        "            j = idxs[pos]\n",
        "\n",
        "            # find the largest (x, y) coordinates for the start of\n",
        "            # the bounding box and the smallest (x, y) coordinates\n",
        "            # for the end of the bounding box\n",
        "            xx1 = max(x1[i], x1[j])\n",
        "            yy1 = max(y1[i], y1[j])\n",
        "            xx2 = min(x2[i], x2[j])\n",
        "            yy2 = min(y2[i], y2[j])\n",
        "\n",
        "            # compute the width and height of the bounding box\n",
        "            w = max(0, xx2 - xx1 + 1)\n",
        "            h = max(0, yy2 - yy1 + 1)\n",
        "\n",
        "            # compute the ratio of overlap between the computed\n",
        "            # bounding box and the bounding box in the area list\n",
        "            overlap = float(w * h) / area[j]\n",
        "\n",
        "            # if there is sufficient overlap, suppress the\n",
        "            # current bounding box\n",
        "            if overlap > overlapThresh:\n",
        "                suppress.append(pos)\n",
        "\n",
        "        # delete all indexes from the index list that are in the\n",
        "        # suppression list\n",
        "        idxs = np.delete(idxs, suppress)\n",
        "    # return only the bounding boxes that were picked\n",
        "    return boxes[pick]\n",
        "\n",
        "\n",
        "def uneBoundingBoxes(boxesAllXmls):\n",
        "    boundingBox=[]\n",
        "    listBox = []\n",
        "    l=len(boxesAllXmls)\n",
        "\n",
        "    while(l>0):\n",
        "        boxPrim=boxesAllXmls[0]\n",
        "\n",
        "        listBox.append(boxPrim)\n",
        "        boxesAllXmls1=boxesAllXmls[1:]\n",
        "        boxesAllXmls.remove(boxPrim)\n",
        "        for box in boxesAllXmls1:\n",
        "            if bb_intersection_over_union(boxPrim, box) > 0.8:\n",
        "                listBox.append(box)\n",
        "                boxesAllXmls.remove(box)\n",
        "\n",
        "        boundingBox.append(listBox)\n",
        "        listBox = []\n",
        "        l=len(boxesAllXmls)\n",
        "        \n",
        "    return boundingBox\n",
        "\n",
        "\n",
        "def bb_intersection_over_union(boxA, boxB):\n",
        "    # determine the (x, y)-coordinates of the intersection rectangle\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2])\n",
        "    yB = min(boxA[3], boxB[3])\n",
        "\n",
        "    # compute the area of intersection rectangle\n",
        "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
        "\n",
        "    # compute the area of both the prediction and ground-truth\n",
        "    # rectangles\n",
        "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
        "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
        "\n",
        "    # compute the intersection over union by taking the intersection\n",
        "    # area and dividing it by the sum of prediction + ground-truth\n",
        "    # areas - the interesection area\n",
        "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
        "\n",
        "    # return the intersection over union value\n",
        "    return iou\n",
        "\n",
        "def ensemble(boxes, option='consensus'):\n",
        "    numFich = 4\n",
        "    result = []\n",
        "    box = uneBoundingBoxes(boxes)\n",
        "\n",
        "    for rectangles in box:\n",
        "        list1 = []\n",
        "        for rc in rectangles:\n",
        "            list1.append(rc)\n",
        "        pick = []\n",
        "\n",
        "        if option == 'consensus':\n",
        "            if len(np.array(list1))>=math.ceil(numFich/2):\n",
        "                #adjust IoU value here\n",
        "                pick = nonMaximumSuppression(np.array(list1), 0.2)\n",
        "\n",
        "        elif option == 'unanimous':\n",
        "            if len(np.array(list1))==numFich:\n",
        "                #adjust IoU value here\n",
        "                pick = nonMaximumSuppression(np.array(list1), 0.2)\n",
        "\n",
        "        elif option == 'affirmative':\n",
        "            #adjust IoU value here\n",
        "            pick = nonMaximumSuppression(np.array(list1), 0.2)\n",
        "\n",
        "        if len(pick)!=0:\n",
        "            result.append(list(pick[0]))\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1523dea",
      "metadata": {
        "id": "e1523dea"
      },
      "source": [
        "### Adjust voting strategy here\n",
        "Perform Bagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15619dee",
      "metadata": {
        "id": "15619dee"
      },
      "outputs": [],
      "source": [
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "from pprint import pprint\n",
        "import math\n",
        "metric = MeanAveragePrecision()\n",
        "#just bagging\n",
        "# model_faster_res.to(device).eval()\n",
        "# model_retina_res.to(device).eval()\n",
        "# model_faster_mo.to(device).eval()\n",
        "# move model to the right device\n",
        "model_retina_res1.to(device).eval()\n",
        "model_retina_res2.to(device).eval()\n",
        "model_retina_res3.to(device).eval()\n",
        "model_retina_res4.to(device).eval()\n",
        "\n",
        "for i, (img, tar) in enumerate(data_loader_test):\n",
        "    print(i)\n",
        "    #(img1, tar1) , (img2, tar2), (img3, tar3) = imgs[0], imgs[1], imgs[2]\n",
        "    #(img1, tar1) = imgs[0]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        img1 = img[0].to(device)\n",
        "        tar1 = tar[0]\n",
        "#         img2 = img2[0].to(device)\n",
        "#         tar2 = tar2[0]\n",
        "#         img3 = img3[0].to(device)\n",
        "#         tar3 = tar3[0]\n",
        "        pred1_1 = model_retina_res1([img1])\n",
        "        pred1_2 = model_retina_res2([img1])\n",
        "        pred1_3 = model_retina_res3([img1])\n",
        "        pred1_4 = model_retina_res4([img1])\n",
        "\n",
        "#         pred2_1 = model_retina_res([img2])\n",
        "#         pred2_2 = model_faster_res([img2])\n",
        "#         pred2_3 = model_faster_mo([img2])\n",
        "        \n",
        "#         pred3_1 = model_retina_res([img3])\n",
        "#         pred3_2 = model_faster_res([img3])\n",
        "#         pred3_3 = model_faster_mo([img3])\n",
        "        \n",
        "        box1_1 = pred1_1[0]['boxes'].cpu().numpy().tolist()\n",
        "        box1_2 = pred1_2[0]['boxes'].cpu().numpy().tolist()\n",
        "        box1_3 = pred1_3[0]['boxes'].cpu().numpy().tolist()\n",
        "        box1_4 = pred1_4[0]['boxes'].cpu().numpy().tolist()\n",
        "\n",
        "#         box2_1 = pred2_1[0]['boxes'].cpu().numpy().tolist()\n",
        "#         box2_2 = pred2_2[0]['boxes'].cpu().numpy().tolist()\n",
        "#         box2_3 = pred2_3[0]['boxes'].cpu().numpy().tolist()\n",
        "        \n",
        "#         box3_1 = pred3_1[0]['boxes'].cpu().numpy().tolist()\n",
        "#         box3_2 = pred3_2[0]['boxes'].cpu().numpy().tolist()\n",
        "#         box3_3 = pred3_3[0]['boxes'].cpu().numpy().tolist()\n",
        "        #boxes = box1_1+box1_2+box1_3+box2_1+box2_2+box2_3+box3_1+box3_2+box3_3\n",
        "        boxes = box1_1+box1_2+box1_3+box1_4\n",
        "        #print(boxes)\n",
        "        #boxes = np.concatenate((box1_1, box1_2, box1_3, box2_1, box2_2, box2_3, box3_1, box3_2, box3_3), axis=0)\n",
        "        #print(boxes)\n",
        "        score1_1 = pred1_1[0]['scores'].cpu().numpy()\n",
        "        score1_2 = pred1_2[0]['scores'].cpu().numpy()\n",
        "        score1_3 = pred1_3[0]['scores'].cpu().numpy()  \n",
        "        score1_4 = pred1_4[0]['scores'].cpu().numpy()\n",
        "\n",
        "        \n",
        "#         score2_1 = pred2_1[0]['scores'].cpu().numpy()\n",
        "#         score2_2 = pred2_2[0]['scores'].cpu().numpy()\n",
        "#         score2_3 = pred2_3[0]['scores'].cpu().numpy()\n",
        "        \n",
        "#         score3_1 = pred3_1[0]['scores'].cpu().numpy()\n",
        "#         score3_2 = pred3_2[0]['scores'].cpu().numpy()\n",
        "#         score3_3 = pred3_3[0]['scores'].cpu().numpy()\n",
        "        \n",
        "        #scores = np.concatenate((score1_1, score1_2, score1_3, score2_1, score2_2, score2_3, score3_1, score3_2, score3_3), 0)\n",
        "        scores = np.concatenate((score1_1, score1_2, score1_3,score1_4), 0)\n",
        "\n",
        "        #print(scores)\n",
        "        boxx = boxes.copy()\n",
        "        #Addjust voting strategy here\n",
        "        #pick = ensemble(boxes, option='consensus')\n",
        "        #pick = ensemble(boxes, option='affirmative')\n",
        "        pick = ensemble(boxes, option='unanimous')\n",
        "\n",
        "        #print(len(pick))\n",
        "        idx = []\n",
        "        boxes = boxx\n",
        "        for j in range(len(pick)):\n",
        "            for k in range(len(boxes)):\n",
        "                if (pick[j] == boxes[k]):#.all():\n",
        "                    idx.append(k)\n",
        "                    break\n",
        "        #idx = np.array(idx)\n",
        "        #print(idx)\n",
        "        #print('idx',pick)\n",
        "        if len(idx)==0:\n",
        "            continue\n",
        "        pick = torch.from_numpy(np.array(pick))\n",
        "        print('am I here')\n",
        "        img = torchvision.transforms.ConvertImageDtype(torch.uint8)(img1.cpu())\n",
        "        result = torchvision.utils.draw_bounding_boxes(img, pick)\n",
        "        out = Image.fromarray(result.permute(1, 2, 0).contiguous().numpy())\n",
        "\n",
        "       # out.save('imgs/test_')\n",
        "        out.show()\n",
        "        \n",
        "        prediction = [{\n",
        "            'boxes' : pick.to(device),\n",
        "            'scores' : torch.from_numpy(scores[idx]).to(device),\n",
        "            'labels' : torch.ones(len(pick)).to(device),\n",
        "        }]\n",
        "        target = [{\n",
        "            'boxes' : tar1['boxes'].to(device),\n",
        "            'labels' : tar1['labels'].to(device),\n",
        "        }]\n",
        "\n",
        "    metric.update(prediction, target)\n",
        "    pprint(metric.compute())\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ead73130",
      "metadata": {
        "id": "ead73130"
      },
      "source": [
        "Bagging + TTA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "addb3254",
      "metadata": {
        "id": "addb3254"
      },
      "outputs": [],
      "source": [
        "#1. \n",
        "dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
        "dataset_test1 = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
        "dataset_test2 = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
        "dataset_test3 = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
        "\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
        "dataset_test1 = torch.utils.data.Subset(dataset_test1, indices[-50:])\n",
        "dataset_test2 = torch.utils.data.Subset(dataset_test2, indices[-50:])\n",
        "dataset_test3 = torch.utils.data.Subset(dataset_test3, indices[-50:])\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader_test1 = torch.utils.data.DataLoader(\n",
        "    dataset_test1, batch_size=1, shuffle=False, pin_memory=True,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_test2 = torch.utils.data.DataLoader(\n",
        "    dataset_test2, batch_size=1, shuffle=False, pin_memory=True,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_test3 = torch.utils.data.DataLoader(\n",
        "    dataset_test3, batch_size=1, shuffle=False, pin_memory=True,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=1, shuffle=False, pin_memory=True,\n",
        "    collate_fn=utils.collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43309024",
      "metadata": {
        "id": "43309024"
      },
      "outputs": [],
      "source": [
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "from pprint import pprint\n",
        "import math\n",
        "metric = MeanAveragePrecision()\n",
        "#Run this one\n",
        "# model_faster_res.to(device).eval()\n",
        "# model_retina_res.to(device).eval()\n",
        "# model_faster_mo.to(device).eval()\n",
        "# model_retina_res1.to(device).eval()\n",
        "# model_retina_res2.to(device).eval()\n",
        "# model_retina_res3.to(device).eval()\n",
        "# model_retina_res4.to(device).eval()\n",
        "i = 0\n",
        "for imgs in zip(data_loader_test, data_loader_test2, data_loader_test3,data_loader_test1):\n",
        "    print(i)\n",
        "    \n",
        "    (img1, tar1) , (img2, tar2), (img3, tar3),(img4,tar4) = imgs[0], imgs[1], imgs[2],imgs[3]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        img1 = img1[0].to(device)\n",
        "        tar1 = tar1[0]\n",
        "        img2 = img2[0].to(device)\n",
        "        tar2 = tar2[0]\n",
        "        img3 = img3[0].to(device)\n",
        "        tar3 = tar3[0]\n",
        "        img4 = img4[0].to(device)\n",
        "        tar4 = tar4[0]\n",
        "        \n",
        "        pred1_1 = model_retina_res1([img1])\n",
        "        pred1_2 = model_retina_res2([img1])\n",
        "        pred1_3 = model_retina_res3([img1])\n",
        "        pred1_4 = model_retina_res4([img1])\n",
        "        \n",
        "        pred2_1 = model_retina_res1([img2])\n",
        "        pred2_2 = model_retina_res2([img2])\n",
        "        pred2_3 = model_retina_res3([img2])\n",
        "        pred2_4 = model_retina_res4([img2])\n",
        "        \n",
        "        pred3_1 = model_retina_res1([img3])\n",
        "        pred3_2 = model_retina_res2([img3])\n",
        "        pred3_3 = model_retina_res3([img3])\n",
        "        pred3_4 = model_retina_res4([img3])\n",
        "        \n",
        "        pred4_1 = model_retina_res1([img4])\n",
        "        pred4_2 = model_retina_res2([img4])\n",
        "        pred4_3 = model_retina_res3([img4])\n",
        "        pred4_4 = model_retina_res4([img4])\n",
        "        \n",
        "#         pred1_1 = model_retina_res([img1])\n",
        "#         pred1_2 = model_faster_res([img1])\n",
        "#         pred1_3 = model_faster_mo([img1])\n",
        "        \n",
        "#         pred2_1 = model_retina_res([img2])\n",
        "#         pred2_2 = model_faster_res([img2])\n",
        "#         pred2_3 = model_faster_mo([img2])\n",
        "        \n",
        "#         pred3_1 = model_retina_res([img3])\n",
        "#         pred3_2 = model_faster_res([img3])\n",
        "#         pred3_3 = model_faster_mo([img3])\n",
        "        \n",
        "        box1_1 = pred1_1[0]['boxes'].cpu().numpy().tolist()\n",
        "        box1_2 = pred1_2[0]['boxes'].cpu().numpy().tolist()\n",
        "        box1_3 = pred1_3[0]['boxes'].cpu().numpy().tolist()\n",
        "        box1_4 = pred1_4[0]['boxes'].cpu().numpy().tolist()\n",
        "        \n",
        "        box2_1 = pred2_1[0]['boxes'].cpu().numpy().tolist()\n",
        "        box2_2 = pred2_2[0]['boxes'].cpu().numpy().tolist()\n",
        "        box2_3 = pred2_3[0]['boxes'].cpu().numpy().tolist()\n",
        "        box2_4 = pred2_4[0]['boxes'].cpu().numpy().tolist()\n",
        "\n",
        "        box3_1 = pred3_1[0]['boxes'].cpu().numpy().tolist()\n",
        "        box3_2 = pred3_2[0]['boxes'].cpu().numpy().tolist()\n",
        "        box3_3 = pred3_3[0]['boxes'].cpu().numpy().tolist()\n",
        "        box3_4 = pred3_4[0]['boxes'].cpu().numpy().tolist()\n",
        "        \n",
        "        box4_1 = pred3_1[0]['boxes'].cpu().numpy().tolist()\n",
        "        box4_2 = pred3_2[0]['boxes'].cpu().numpy().tolist()\n",
        "        box4_3 = pred3_3[0]['boxes'].cpu().numpy().tolist()\n",
        "        box4_4 = pred3_4[0]['boxes'].cpu().numpy().tolist()\n",
        "\n",
        "        boxes = box1_1+box1_2+box1_3+box1_4+box2_1+box2_2+box2_3+box2_4+box3_1+box3_2+box3_3+box3_4+box4_1+box4_2+box4_3+box4_4\n",
        "        #print(boxes)\n",
        "        #boxes = np.concatenate((box1_1, box1_2, box1_3, box2_1, box2_2, box2_3, box3_1, box3_2, box3_3), axis=0)\n",
        "        #print(boxes)\n",
        "        score1_1 = pred1_1[0]['scores'].cpu().numpy()\n",
        "        score1_2 = pred1_2[0]['scores'].cpu().numpy()\n",
        "        score1_3 = pred1_3[0]['scores'].cpu().numpy()\n",
        "        score1_4 = pred1_4[0]['scores'].cpu().numpy()\n",
        "\n",
        "        score2_1 = pred2_1[0]['scores'].cpu().numpy()\n",
        "        score2_2 = pred2_2[0]['scores'].cpu().numpy()\n",
        "        score2_3 = pred2_3[0]['scores'].cpu().numpy()\n",
        "        score2_4 = pred2_4[0]['scores'].cpu().numpy()\n",
        "\n",
        "        score3_1 = pred3_1[0]['scores'].cpu().numpy()\n",
        "        score3_2 = pred3_2[0]['scores'].cpu().numpy()\n",
        "        score3_3 = pred3_3[0]['scores'].cpu().numpy()\n",
        "        score3_4 = pred3_4[0]['scores'].cpu().numpy()\n",
        "        \n",
        "        score4_1 = pred4_1[0]['scores'].cpu().numpy()\n",
        "        score4_2 = pred4_2[0]['scores'].cpu().numpy()\n",
        "        score4_3 = pred4_3[0]['scores'].cpu().numpy()\n",
        "        score4_4 = pred4_4[0]['scores'].cpu().numpy()\n",
        "\n",
        "        \n",
        "        scores = np.concatenate((score1_1, score1_2,score1_3,score1_4, score2_1, score2_2, score2_3,score2_4, score3_1, score3_2, score3_3, score3_4, score4_1, score4_2, score4_3, score4_4), 0)\n",
        "        #print(scores)\n",
        "        boxx = boxes.copy()\n",
        "        #pick = ensemble(boxes, option='consensus')\n",
        "        #pick = ensemble(boxes, option='affirmative')\n",
        "        pick = ensemble(boxes, option='unanimous')\n",
        "\n",
        "        #print(len(pick))\n",
        "        idx = []\n",
        "        boxes = boxx\n",
        "        for j in range(len(pick)):\n",
        "            for k in range(len(boxes)):\n",
        "                if (pick[j] == boxes[k]):#.all():\n",
        "                    idx.append(k)\n",
        "                    break\n",
        "        #idx = np.array(idx)\n",
        "        #print(idx)\n",
        "        if len(idx)==0:\n",
        "            continue\n",
        "        pick = torch.from_numpy(np.array(pick))\n",
        "\n",
        "        img = torchvision.transforms.ConvertImageDtype(torch.uint8)(img1.cpu())\n",
        "        result = torchvision.utils.draw_bounding_boxes(img, pick)\n",
        "        out = Image.fromarray(result.permute(1, 2, 0).contiguous().numpy())\n",
        "\n",
        "       # out.save('imgs/test_')\n",
        "        out.show()\n",
        "        \n",
        "        prediction = [{\n",
        "            'boxes' : pick.to(device),\n",
        "            'scores' : torch.from_numpy(scores[idx]).to(device),\n",
        "            'labels' : torch.ones(len(pick)).to(device),\n",
        "        }]\n",
        "        target = [{\n",
        "            'boxes' : tar1['boxes'].to(device),\n",
        "            'labels' : tar1['labels'].to(device),\n",
        "        }]\n",
        "\n",
        "    metric.update(prediction, target)\n",
        "    pprint(metric.compute())\n",
        "    i += 1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "bagging2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}