{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71821eb",
   "metadata": {
    "id": "f71821eb"
   },
   "outputs": [],
   "source": [
    "#!unzip PennFudanPed.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cd93dd",
   "metadata": {
    "id": "f2cd93dd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import math\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import RetinaNet\n",
    "from engine_my import train_one_epoch, evaluate\n",
    "import utils\n",
    "#import wandb\n",
    "\n",
    "          \n",
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        mask = Image.open(mask_path)\n",
    "\n",
    "        mask = np.array(mask)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "dataset = PennFudanDataset('PennFudanPed/')\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # during testing, randomly colour imgs for data augmentation\n",
    "        transforms.append(T.ColorJitter(hue=(-0.4, 0.4)))\n",
    "#         transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "\n",
    "# use our dataset and defined transformations\n",
    "dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
    "dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, pin_memory=True,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, pin_memory=True,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fc65d7",
   "metadata": {
    "id": "80fc65d7"
   },
   "source": [
    "### retinanet with resnet50 backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d5b77d",
   "metadata": {
    "id": "69d5b77d",
    "outputId": "d52a572a-5b5c-4a7b-d313-20189b8f90f4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#wandb.init(project='228_project', name=\"retina_res\")\n",
    "model_retina_res = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True)\n",
    "# move model to the right device\n",
    "model_retina_res.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model_retina_res.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)\n",
    "\n",
    "for epoch in range(10):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model_retina_res, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model_retina_res, data_loader_test, device=device)\n",
    "    \n",
    "# evaluate(model_retina_res, data_loader, device=device)\n",
    "# evaluate(model_retina_res, data_loader_test, device=device)\n",
    "\n",
    "# pick one image from the test set\n",
    "# img, _ = dataset_test[0]\n",
    "# # put the model in evaluation mode\n",
    "# model_retina_res.eval()\n",
    "# with torch.no_grad():\n",
    "#     prediction1 = model_retina_res([img.to(device)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da08dad6",
   "metadata": {
    "id": "da08dad6"
   },
   "source": [
    "### MEAN-AVERAGE-PRECISION (MAP)\n",
    "https://torchmetrics.readthedocs.io/en/stable/detection/mean_average_precision.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3a948a",
   "metadata": {
    "id": "ca3a948a"
   },
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/PytorchLightning/metrics.git@release/latest\n",
    "# from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "# from pprint import pprint\n",
    "# metric = MeanAveragePrecision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf431a85",
   "metadata": {
    "id": "bf431a85"
   },
   "outputs": [],
   "source": [
    "\n",
    "# for i in range(len(dataset_test)):\n",
    "    \n",
    "#     img, tar = dataset_test[i]\n",
    "#     target = [{\n",
    "#         'boxes' : tar['boxes'].to(device),\n",
    "#         'labels' : tar['labels'].to(device),\n",
    "#         }]\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         prediction = model_retina_res([img.to(device)])\n",
    "\n",
    "#     metric.update(prediction, target)\n",
    "#     pprint(metric.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ee8edd",
   "metadata": {
    "id": "a9ee8edd"
   },
   "outputs": [],
   "source": [
    "# for test in dataset_test:\n",
    "#     img, tar = test\n",
    "#     target = [{\n",
    "#         'boxes' : tar['boxes'].to(device),\n",
    "#         'labels' : tar['labels'].to(device),\n",
    "#         }]\n",
    "#     prediction = model_retina_res([img.to(device)])\n",
    "#     metric.update(prediction, target)\n",
    "#     pprint(metric.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d12138",
   "metadata": {
    "id": "76d12138"
   },
   "source": [
    "### Faser rcnn with resnet50 backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e44dab7",
   "metadata": {
    "id": "9e44dab7",
    "outputId": "1cb25d5a-f375-46d1-979e-b0114e12fc0a"
   },
   "outputs": [],
   "source": [
    "#wandb.init(project='228_project', name=\"faster_res\")\n",
    "model_faster_res = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model_faster_res.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model_faster_res.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)\n",
    "\n",
    "for epoch in range(10):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model_faster_res, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model_faster_res, data_loader_test, device=device)\n",
    "\n",
    "# pick one image from the test set\n",
    "# img, _ = dataset_test[0]\n",
    "# # put the model in evaluation mode\n",
    "# model_faster_res.eval()\n",
    "# with torch.no_grad():\n",
    "#     prediction2 = model_faster_res([img.to(device)])\n",
    "\n",
    "# print(\"print prediction\")\n",
    "# print(prediction2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0b227d",
   "metadata": {
    "id": "3e0b227d"
   },
   "source": [
    "### Faser rcnn with mobilenet backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeafc01f",
   "metadata": {
    "id": "aeafc01f",
    "outputId": "08a88de6-ded2-4eaa-d8f6-819669598e03"
   },
   "outputs": [],
   "source": [
    "#wandb.init(project='228_project', name=\"faster_mo\")\n",
    "model_faster_mo = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
    "model_faster_mo.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model_faster_mo.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)\n",
    "\n",
    "\n",
    "for epoch in range(10):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model_faster_mo, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model_faster_mo, data_loader_test, device=device)\n",
    "    \n",
    "# pick one image from the test set\n",
    "# img, _ = dataset_test[0]\n",
    "# # put the model in evaluation mode\n",
    "# model_faster_mo.eval()\n",
    "# with torch.no_grad():\n",
    "#     prediction3 = model_faster_mo([img.to(device)])\n",
    "\n",
    "# print(\"print prediction\")\n",
    "# print(prediction3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdacc64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models\n",
    "torch.save(model_retina_res.state_dict(), 'models/retina_res.pth')\n",
    "torch.save(model_faster_res.state_dict(), 'models/faster_res.pth')\n",
    "torch.save(model_faster_mo.state_dict(), 'models/faster_mo.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92351bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models\n",
    "model_retina_res = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=False)\n",
    "model_retina_res.load_state_dict(torch.load('models/retina_res.pth'))\n",
    "\n",
    "model_faster_res = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
    "model_faster_res.load_state_dict(torch.load('models/faster_res.pth'))\n",
    "\n",
    "model_faster_mo = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=False)\n",
    "model_faster_mo.load_state_dict(torch.load('models/faster_mo.pth'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549d4595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonMaximumSuppression(boxes, overlapThresh):\n",
    "    # if there are no boxes, return an empty list\n",
    "\n",
    "    # initialize the list of picked indexes\n",
    "    pick = []\n",
    "    probFinal = 0\n",
    "    # grab the coordinates of the bounding boxes\n",
    "    x1 = boxes[:, 0].astype(float)\n",
    "    y1 = boxes[:, 1].astype(float)\n",
    "    x2 = boxes[:, 2].astype(float)\n",
    "    y2 = boxes[:, 3].astype(float)\n",
    "\n",
    "    # compute the area of the bounding boxes and sort the bounding\n",
    "    # boxes by the bottom-right y-coordinate of the bounding box\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = np.argsort(y2)\n",
    "    # keep looping while some indexes still remain in the indexes\n",
    "    # list\n",
    "    while len(idxs) > 0:\n",
    "        # grab the last index in the indexes list, add the index\n",
    "        # value to the list of picked indexes, then initialize\n",
    "        # the suppression list (i.e. indexes that will be deleted)\n",
    "        # using the last index\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    "        suppress = [last]\n",
    "        # loop over all indexes in the indexes list\n",
    "        for pos in range(0, last):\n",
    "            # grab the current index\n",
    "            j = idxs[pos]\n",
    "\n",
    "            # find the largest (x, y) coordinates for the start of\n",
    "            # the bounding box and the smallest (x, y) coordinates\n",
    "            # for the end of the bounding box\n",
    "            xx1 = max(x1[i], x1[j])\n",
    "            yy1 = max(y1[i], y1[j])\n",
    "            xx2 = min(x2[i], x2[j])\n",
    "            yy2 = min(y2[i], y2[j])\n",
    "\n",
    "            # compute the width and height of the bounding box\n",
    "            w = max(0, xx2 - xx1 + 1)\n",
    "            h = max(0, yy2 - yy1 + 1)\n",
    "\n",
    "            # compute the ratio of overlap between the computed\n",
    "            # bounding box and the bounding box in the area list\n",
    "            overlap = float(w * h) / area[j]\n",
    "\n",
    "            # if there is sufficient overlap, suppress the\n",
    "            # current bounding box\n",
    "            if overlap > overlapThresh:\n",
    "                suppress.append(pos)\n",
    "\n",
    "        # delete all indexes from the index list that are in the\n",
    "        # suppression list\n",
    "        idxs = np.delete(idxs, suppress)\n",
    "    # return only the bounding boxes that were picked\n",
    "    return boxes[pick]\n",
    "\n",
    "\n",
    "def uneBoundingBoxes(boxesAllXmls):\n",
    "    boundingBox=[]\n",
    "    listBox = []\n",
    "    l=len(boxesAllXmls)\n",
    "\n",
    "    while(l>0):\n",
    "        boxPrim=boxesAllXmls[0]\n",
    "\n",
    "        listBox.append(boxPrim)\n",
    "        boxesAllXmls1=boxesAllXmls[1:]\n",
    "        boxesAllXmls.remove(boxPrim)\n",
    "        for box in boxesAllXmls1:\n",
    "            if bb_intersection_over_union(boxPrim, box) > 0.5:\n",
    "                listBox.append(box)\n",
    "                boxesAllXmls.remove(box)\n",
    "\n",
    "        boundingBox.append(listBox)\n",
    "        listBox = []\n",
    "        l=len(boxesAllXmls)\n",
    "        \n",
    "    return boundingBox\n",
    "\n",
    "\n",
    "def bb_intersection_over_union(boxA, boxB):\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "    # return the intersection over union value\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e261387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(boxes, option='consensus'):\n",
    "    numFich = 3\n",
    "    result = []\n",
    "    box = uneBoundingBoxes(boxes)\n",
    "\n",
    "    for rectangles in box:\n",
    "        list1 = []\n",
    "        for rc in rectangles:\n",
    "            list1.append(rc)\n",
    "        pick = []\n",
    "\n",
    "        if option == 'consensus':\n",
    "            if len(np.array(list1))>=math.ceil(numFich/2):\n",
    "                pick = nonMaximumSuppression(np.array(list1), 0.3)\n",
    "\n",
    "        elif option == 'unanimous':\n",
    "            if len(np.array(list1))==numFich:\n",
    "                pick = nonMaximumSuppression(np.array(list1), 0.3)\n",
    "\n",
    "        elif option == 'affirmative':\n",
    "            pick = nonMaximumSuppression(np.array(list1), 0.3)\n",
    "\n",
    "        if len(pick)!=0:\n",
    "            result.append(list(pick[0]))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493bd839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from pprint import pprint\n",
    "metric = MeanAveragePrecision()\n",
    "\n",
    "model_faster_res.to(device).eval()\n",
    "model_retina_res.to(device).eval()\n",
    "model_faster_mo.to(device).eval()\n",
    "cnt = 1\n",
    "for i, (img, tar) in enumerate(data_loader_test):\n",
    "    print(i)\n",
    "    with torch.no_grad():\n",
    "        img = img[0].to(device)\n",
    "        tar = tar[0]\n",
    "        pred1 = model_retina_res([img])\n",
    "        pred2 = model_faster_res([img])\n",
    "        pred3 = model_faster_mo([img])\n",
    "        \n",
    "        box1 = pred1[0]['boxes'].cpu().numpy().tolist()\n",
    "        box2 = pred2[0]['boxes'].cpu().numpy().tolist()\n",
    "        box3 = pred3[0]['boxes'].cpu().numpy().tolist()\n",
    "        boxes = box1+box2+box3\n",
    "        \n",
    "        score1 = pred1[0]['scores'].cpu().numpy()\n",
    "        score2 = pred2[0]['scores'].cpu().numpy()\n",
    "        score3 = pred3[0]['scores'].cpu().numpy()\n",
    "        scores = np.concatenate((score1, score2, score3), 0)\n",
    "        \n",
    "        boxx = boxes.copy()\n",
    "        pick = ensemble(boxes, option='consensus')\n",
    "        boxes = boxx\n",
    "\n",
    "        idx = []\n",
    "\n",
    "        for j in range(len(pick)):\n",
    "            for k in range(len(boxes)):\n",
    "                if (pick[j] == boxes[k]):#.all():\n",
    "                    idx.append(k)\n",
    "                    break\n",
    "        idx = np.array(idx)\n",
    "        \n",
    "        pick = torch.from_numpy(np.array(pick))\n",
    "        img = torchvision.transforms.ConvertImageDtype(torch.uint8)(img.cpu())\n",
    "        result = torchvision.utils.draw_bounding_boxes(img, pick)\n",
    "        out = Image.fromarray(result.permute(1, 2, 0).contiguous().numpy())\n",
    "        out.show()\n",
    "        \n",
    "        prediction = [{\n",
    "            'boxes' : pick.to(device),\n",
    "            'scores' : torch.from_numpy(scores[idx]).to(device),\n",
    "            'labels' : torch.ones(len(pick)).to(device),\n",
    "        }]\n",
    "        target = [{\n",
    "            'boxes' : tar['boxes'].to(device),\n",
    "            'labels' : tar['labels'].to(device),\n",
    "        }]\n",
    "\n",
    "    metric.update(prediction, target)\n",
    "    pprint(metric.compute())\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b8a307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from pprint import pprint\n",
    "metric = MeanAveragePrecision()\n",
    "\n",
    "for i in range(len(dataset_test)):\n",
    "    \n",
    "    img, tar = dataset_test[i]\n",
    "    target = [{\n",
    "        'boxes' : tar['boxes'].to(device),\n",
    "        'labels' : tar['labels'].to(device),\n",
    "        }]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model_retina_res([img.to(device)])\n",
    "\n",
    "    metric.update(prediction, target)\n",
    "    pprint(metric.compute())\n",
    "    \n",
    "# for i in range(1):\n",
    "#     img, tar = dataset_test[i]\n",
    "#     target = [{\n",
    "#         'boxes' : tar['boxes'].to(device),\n",
    "#         'labels' : tar['labels'].to(device),\n",
    "#         }]\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         prediction = model_retina_res([img.to(device)])\n",
    "#     print(prediction)\n",
    "#     print(target)\n",
    "#     metric.update(prediction, target)\n",
    "#     pprint(metric.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0322621",
   "metadata": {},
   "source": [
    "### Evaluate our models by COCOAPI metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3486f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "\n",
    "import torchvision.models.detection.mask_rcnn\n",
    "from coco_utils import get_coco_api_from_dataset\n",
    "from coco_eval_my import CocoEvaluator\n",
    "import utils\n",
    "\n",
    "def _get_iou_types(model):\n",
    "    model_without_ddp = model\n",
    "    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
    "        model_without_ddp = model.module\n",
    "    iou_types = [\"bbox\"]\n",
    "    if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n",
    "        iou_types.append(\"segm\")\n",
    "    if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):\n",
    "        iou_types.append(\"keypoints\")\n",
    "    return iou_types\n",
    "\n",
    "@torch.no_grad()\n",
    "def my_eval(model, data_loader, device):\n",
    "    n_threads = torch.get_num_threads()\n",
    "    # FIXME remove this and make paste_masks_in_image run on the GPU\n",
    "    torch.set_num_threads(1)\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    model.eval()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    header = 'Test:'\n",
    "\n",
    "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
    "    iou_types = _get_iou_types(model)\n",
    "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, 100, header):\n",
    "        images = list(img.to(device) for img in images)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        model_time = time.time()\n",
    "        outputs = model(images)\n",
    "\n",
    "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "        model_time = time.time() - model_time\n",
    "\n",
    "        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
    "        evaluator_time = time.time()\n",
    "        coco_evaluator.update(res)\n",
    "        evaluator_time = time.time() - evaluator_time\n",
    "        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    coco_evaluator.synchronize_between_processes()\n",
    "\n",
    "    # accumulate predictions from all images\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n",
    "    torch.set_num_threads(n_threads)\n",
    "    return coco_evaluator\n",
    "\n",
    "my_eval(model_retina_res, data_loader_test, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126e4ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "all_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
