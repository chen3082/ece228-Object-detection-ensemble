{"cells":[{"cell_type":"code","execution_count":null,"id":"f71821eb","metadata":{"id":"f71821eb"},"outputs":[],"source":["#!unzip PennFudanPed.zip"]},{"cell_type":"code","execution_count":null,"id":"f2cd93dd","metadata":{"id":"f2cd93dd"},"outputs":[],"source":["import os\n","import numpy as np\n","import torch\n","import torch.utils.data\n","from PIL import Image\n","from torch.optim.lr_scheduler import StepLR\n","import timm\n","import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection import RetinaNet\n","from engine_my import train_one_epoch, evaluate\n","import utils\n","import torchvision.transforms as T\n","\n","class PennFudanDataset(torch.utils.data.Dataset):\n","    def __init__(self, root, transforms=None):\n","        self.root = root\n","        self.transforms = transforms\n","        # load all image files, sorting them to\n","        # ensure that they are aligned\n","        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n","        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n","\n","    def __getitem__(self, idx):\n","        # load images ad masks\n","        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n","        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n","        img = Image.open(img_path).convert(\"RGB\")\n","        # note that we haven't converted the mask to RGB,\n","        # because each color corresponds to a different instance\n","        # with 0 being background\n","        mask = Image.open(mask_path)\n","\n","        mask = np.array(mask)\n","        # instances are encoded as different colors\n","        obj_ids = np.unique(mask)\n","        # first id is the background, so remove it\n","        obj_ids = obj_ids[1:]\n","\n","        # split the color-encoded mask into a set\n","        # of binary masks\n","        masks = mask == obj_ids[:, None, None]\n","\n","        # get bounding box coordinates for each mask\n","        num_objs = len(obj_ids)\n","        boxes = []\n","        for i in range(num_objs):\n","            pos = np.where(masks[i])\n","            xmin = np.min(pos[1])\n","            xmax = np.max(pos[1])\n","            ymin = np.min(pos[0])\n","            ymax = np.max(pos[0])\n","            boxes.append([xmin, ymin, xmax, ymax])\n","\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        # there is only one class\n","        labels = torch.ones((num_objs,), dtype=torch.int64)\n","        masks = torch.as_tensor(masks, dtype=torch.uint8)\n","\n","        image_id = torch.tensor([idx])\n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","        # suppose all instances are not crowd\n","        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n","\n","        target = {}\n","        target[\"boxes\"] = boxes\n","        target[\"labels\"] = labels\n","        target[\"masks\"] = masks\n","        target[\"image_id\"] = image_id\n","        target[\"area\"] = area\n","        target[\"iscrowd\"] = iscrowd\n","\n","        if self.transforms is not None:\n","            img = self.transforms(img)\n","\n","        return img, target\n","\n","    def __len__(self):\n","        return len(self.imgs)\n","\n","\n","def get_transform(train):\n","    transforms = []\n","    # converts the image, a PIL image, into a PyTorch Tensor\n","    transforms.append(T.ToTensor())\n","    if train:\n","        # during testing, randomly colour imgs for data augmentation\n","        transforms.append(T.ColorJitter(hue=(-0.4, 0.4)))\n","#         transforms.append(T.RandomHorizontalFlip(0.5))\n","    return T.Compose(transforms)\n","\n","# use our dataset and defined transformations\n","dataset = PennFudanDataset('PennFudanPed', get_transform(train=False))\n","dataset2 = PennFudanDataset('PennFudanPed', get_transform(train=True))\n","dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n","dataset_test_no = PennFudanDataset('PennFudanPed', get_transform(train=False))\n","\n","# split the dataset in train and test set\n","torch.manual_seed(1)\n","indices = torch.randperm(len(dataset)).tolist()\n","dataset = torch.utils.data.Subset(dataset, indices[:-50])\n","dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n","dataset_test_no = torch.utils.data.Subset(dataset_test_no, indices[-50:])\n","\n","\n","# define training and validation data loaders\n","data_loader = torch.utils.data.DataLoader(\n","    dataset, batch_size=2, shuffle=True, pin_memory=True,\n","    collate_fn=utils.collate_fn)\n","\n","data_loader2 = torch.utils.data.DataLoader(\n","    dataset2, batch_size=2, shuffle=True, pin_memory=True,\n","    collate_fn=utils.collate_fn)\n","\n","data_loader_test = torch.utils.data.DataLoader(\n","    dataset_test, batch_size=1, shuffle=False, pin_memory=True,\n","    collate_fn=utils.collate_fn)\n","\n","data_loader_test_no = torch.utils.data.DataLoader(\n","    dataset_test_no, batch_size=1, shuffle=False, pin_memory=True,\n","    collate_fn=utils.collate_fn)\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","# our dataset has two classes only - background and person\n","num_classes = 2"]},{"cell_type":"code","execution_count":null,"id":"7d8ec8af","metadata":{"id":"7d8ec8af"},"outputs":[],"source":["#F1 score = (Precision Ã— Recall)/[(Precision + Recall)/2]\n"]},{"cell_type":"markdown","id":"5NRWgs5XNWKg","metadata":{"id":"5NRWgs5XNWKg"},"source":["### Load the models weights we've already trained.\n","### (The ones with colouring transformation on training dataset)"]},{"cell_type":"code","execution_count":null,"id":"92351bfb","metadata":{"id":"92351bfb"},"outputs":[],"source":["# load models\n","model_retina_res = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=False)\n","model_retina_res.load_state_dict(torch.load('models/retina_res_aug.pth'))\n","\n","model_faster_res = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n","model_faster_res.load_state_dict(torch.load('models/faster_res_aug.pth'))\n","\n","model_faster_mo = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=False)\n","model_faster_mo.load_state_dict(torch.load('models/faster_mo_aug.pth'))"]},{"cell_type":"code","execution_count":null,"id":"6e261387","metadata":{"id":"6e261387"},"outputs":[],"source":["def ensemble(boxes, option='consensus'):\n","    numFich = 3\n","    result = []\n","    box = uneBoundingBoxes(boxes)\n","\n","    for rectangles in box:\n","        list1 = []\n","        for rc in rectangles:\n","            list1.append(rc)\n","        pick = []\n","\n","        if option == 'consensus':\n","            if len(np.array(list1))>=math.ceil(numFich/2):\n","                pick = nonMaximumSuppression(np.array(list1), 0.5)\n","\n","        elif option == 'unanimous':\n","            if len(np.array(list1))==numFich:\n","                pick = nonMaximumSuppression(np.array(list1), 0.2)\n","\n","        elif option == 'affirmative':\n","            pick = nonMaximumSuppression(np.array(list1), 0.2)\n","\n","        if len(pick)!=0:\n","            result.append(list(pick[0]))\n","\n","    return result\n"]},{"cell_type":"markdown","id":"f5efed36","metadata":{"id":"f5efed36"},"source":["### Do data augmentation experiments"]},{"cell_type":"code","execution_count":null,"id":"dfaa96dc","metadata":{"id":"dfaa96dc"},"outputs":[],"source":["def nonMaximumSuppression(boxes, overlapThresh):\n","    # if there are no boxes, return an empty list\n","\n","    # initialize the list of picked indexes\n","    pick = []\n","    probFinal = 0\n","    # grab the coordinates of the bounding boxes\n","    x1 = boxes[:, 0].astype(float)\n","    y1 = boxes[:, 1].astype(float)\n","    x2 = boxes[:, 2].astype(float)\n","    y2 = boxes[:, 3].astype(float)\n","\n","    # compute the area of the bounding boxes and sort the bounding\n","    # boxes by the bottom-right y-coordinate of the bounding box\n","    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n","    idxs = np.argsort(y2)\n","    # keep looping while some indexes still remain in the indexes\n","    # list\n","    while len(idxs) > 0:\n","        # grab the last index in the indexes list, add the index\n","        # value to the list of picked indexes, then initialize\n","        # the suppression list (i.e. indexes that will be deleted)\n","        # using the last index\n","        last = len(idxs) - 1\n","        i = idxs[last]\n","        pick.append(i)\n","        suppress = [last]\n","        # loop over all indexes in the indexes list\n","        for pos in range(0, last):\n","            # grab the current index\n","            j = idxs[pos]\n","\n","            # find the largest (x, y) coordinates for the start of\n","            # the bounding box and the smallest (x, y) coordinates\n","            # for the end of the bounding box\n","            xx1 = max(x1[i], x1[j])\n","            yy1 = max(y1[i], y1[j])\n","            xx2 = min(x2[i], x2[j])\n","            yy2 = min(y2[i], y2[j])\n","\n","            # compute the width and height of the bounding box\n","            w = max(0, xx2 - xx1 + 1)\n","            h = max(0, yy2 - yy1 + 1)\n","\n","            # compute the ratio of overlap between the computed\n","            # bounding box and the bounding box in the area list\n","            overlap = float(w * h) / area[j]\n","\n","            # if there is sufficient overlap, suppress the\n","            # current bounding box\n","            if overlap > overlapThresh:\n","                suppress.append(pos)\n","\n","        # delete all indexes from the index list that are in the\n","        # suppression list\n","        idxs = np.delete(idxs, suppress)\n","    # return only the bounding boxes that were picked\n","    return boxes[pick]\n","\n","\n","def uneBoundingBoxes(boxesAllXmls):\n","    boundingBox=[]\n","    listBox = []\n","    l=len(boxesAllXmls)\n","\n","    while(l>0):\n","        boxPrim=boxesAllXmls[0]\n","\n","        listBox.append(boxPrim)\n","        boxesAllXmls1=boxesAllXmls[1:]\n","        boxesAllXmls.remove(boxPrim)\n","        for box in boxesAllXmls1:\n","            if bb_intersection_over_union(boxPrim, box) > 0.8:\n","                listBox.append(box)\n","                boxesAllXmls.remove(box)\n","\n","        boundingBox.append(listBox)\n","        listBox = []\n","        l=len(boxesAllXmls)\n","        \n","    return boundingBox\n","\n","\n","def bb_intersection_over_union(boxA, boxB):\n","    # determine the (x, y)-coordinates of the intersection rectangle\n","    xA = max(boxA[0], boxB[0])\n","    yA = max(boxA[1], boxB[1])\n","    xB = min(boxA[2], boxB[2])\n","    yB = min(boxA[3], boxB[3])\n","\n","    # compute the area of intersection rectangle\n","    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n","\n","    # compute the area of both the prediction and ground-truth\n","    # rectangles\n","    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n","    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n","\n","    # compute the intersection over union by taking the intersection\n","    # area and dividing it by the sum of prediction + ground-truth\n","    # areas - the interesection area\n","    iou = interArea / float(boxAArea + boxBArea - interArea)\n","\n","    # return the intersection over union value\n","    return iou"]},{"cell_type":"code","execution_count":null,"id":"493bd839","metadata":{"id":"493bd839"},"outputs":[],"source":["from torchmetrics.detection.mean_ap import MeanAveragePrecision\n","from pprint import pprint\n","import math\n","metric = MeanAveragePrecision()\n","\n","model_faster_res.to(device).eval()\n","model_retina_res.to(device).eval()\n","model_faster_mo.to(device).eval()\n","\n","for i, (img, tar) in enumerate(data_loader_test):\n","    print(i)\n","    with torch.no_grad():\n","        img = img[0].to(device)\n","        tar = tar[0]\n","        pred1 = model_retina_res([img])\n","        pred2 = model_faster_res([img])\n","        pred3 = model_faster_mo([img])\n","        \n","        box1 = pred1[0]['boxes'].cpu().numpy().tolist()\n","        box2 = pred2[0]['boxes'].cpu().numpy().tolist()\n","        box3 = pred3[0]['boxes'].cpu().numpy().tolist()\n","        boxes = box1+box2+box3\n","        \n","        score1 = pred1[0]['scores'].cpu().numpy()\n","        score2 = pred2[0]['scores'].cpu().numpy()\n","        score3 = pred3[0]['scores'].cpu().numpy()\n","        scores = np.concatenate((score1, score2, score3), 0)\n","        \n","        boxx = boxes.copy()\n","        pick = ensemble(boxes, option='unanimous')\n","        boxes = boxx\n","\n","        idx = []\n","\n","        for j in range(len(pick)):\n","            for k in range(len(boxes)):\n","                if (pick[j] == boxes[k]):#.all():\n","                    idx.append(k)\n","                    break\n","        #idx = np.array(idx)\n","        \n","        pick = torch.from_numpy(np.array(pick))\n","        if len(pick) ==0:\n","            continue\n","        img = torchvision.transforms.ConvertImageDtype(torch.uint8)(img.cpu())\n","        result = torchvision.utils.draw_bounding_boxes(img, pick)\n","        out = Image.fromarray(result.permute(1, 2, 0).contiguous().numpy())\n","        if i == 15:\n","            out.save('imgs/img15_2_u.png')\n","        if i == 34:\n","            out.save('imgs/img34_2_u.png')\n","        out.show()\n","        \n","        prediction = [{\n","            'boxes' : pick.to(device),\n","            'scores' : torch.from_numpy(scores[idx]).to(device),\n","            'labels' : torch.ones(len(pick)).to(device),\n","        }]\n","        target = [{\n","            'boxes' : tar['boxes'].to(device),\n","            'labels' : tar['labels'].to(device),\n","        }]\n","\n","    metric.update(prediction, target)\n","    pprint(metric.compute())\n","    print()\n","    "]},{"cell_type":"code","execution_count":null,"id":"9691cfcd","metadata":{"id":"9691cfcd"},"outputs":[],"source":["dataset_test2 = PennFudanDataset('PennFudanPed', get_transform(train=True))\n","dataset_test3 = PennFudanDataset('PennFudanPed', get_transform(train=True))\n","\n","dataset_test2 = torch.utils.data.Subset(dataset_test2, indices[-50:])\n","dataset_test3 = torch.utils.data.Subset(dataset_test3, indices[-50:])\n","\n","data_loader_test2 = torch.utils.data.DataLoader(\n","    dataset_test2, batch_size=1, shuffle=False, pin_memory=True,\n","    collate_fn=utils.collate_fn)\n","data_loader_test3 = torch.utils.data.DataLoader(\n","    dataset_test3, batch_size=1, shuffle=False, pin_memory=True,\n","    collate_fn=utils.collate_fn)"]},{"cell_type":"code","execution_count":null,"id":"44b8a307","metadata":{"id":"44b8a307"},"outputs":[],"source":["from torchmetrics.detection.mean_ap import MeanAveragePrecision\n","from pprint import pprint\n","metric = MeanAveragePrecision()\n","\n","for i in range(len(dataset_test)):\n","    \n","    img, tar = dataset_test[i]\n","    target = [{\n","        'boxes' : tar['boxes'].to(device),\n","        'labels' : tar['labels'].to(device),\n","        }]\n","\n","    with torch.no_grad():\n","        prediction = model_retina_res([img.to(device)])\n","        img = torchvision.transforms.ConvertImageDtype(torch.uint8)(img.cpu())\n","        result = torchvision.utils.draw_bounding_boxes(img, prediction[0]['boxes'])\n","        out = Image.fromarray(result.permute(1, 2, 0).contiguous().numpy())\n","\n","    metric.update(prediction, target)\n","    pprint(metric.compute())\n","    "]},{"cell_type":"code","execution_count":null,"id":"db3486f2","metadata":{"id":"db3486f2"},"outputs":[],"source":["import math\n","import sys\n","import time\n","import torch\n","\n","import torchvision.models.detection.mask_rcnn\n","from coco_utils import get_coco_api_from_dataset\n","from coco_eval_my import CocoEvaluator\n","import utils\n","\n","def _get_iou_types(model):\n","    model_without_ddp = model\n","    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n","        model_without_ddp = model.module\n","    iou_types = [\"bbox\"]\n","    if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n","        iou_types.append(\"segm\")\n","    if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):\n","        iou_types.append(\"keypoints\")\n","    return iou_types\n","\n","@torch.no_grad()\n","def my_eval(model, data_loader, device):\n","    n_threads = torch.get_num_threads()\n","    # FIXME remove this and make paste_masks_in_image run on the GPU\n","    torch.set_num_threads(1)\n","    cpu_device = torch.device(\"cpu\")\n","    model.eval()\n","    metric_logger = utils.MetricLogger(delimiter=\"  \")\n","    header = 'Test:'\n","\n","    coco = get_coco_api_from_dataset(data_loader.dataset)\n","    iou_types = _get_iou_types(model)\n","    coco_evaluator = CocoEvaluator(coco, iou_types)\n","\n","    for images, targets in metric_logger.log_every(data_loader, 100, header):\n","        images = list(img.to(device) for img in images)\n","\n","        torch.cuda.synchronize()\n","        model_time = time.time()\n","        outputs = model(images)\n","\n","        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n","        model_time = time.time() - model_time\n","\n","        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n","        evaluator_time = time.time()\n","        coco_evaluator.update(res)\n","        evaluator_time = time.time() - evaluator_time\n","        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n","\n","    # gather the stats from all processes\n","    metric_logger.synchronize_between_processes()\n","    print(\"Averaged stats:\", metric_logger)\n","    coco_evaluator.synchronize_between_processes()\n","\n","    # accumulate predictions from all images\n","    coco_evaluator.accumulate()\n","    coco_evaluator.summarize()\n","    torch.set_num_threads(n_threads)\n","    return coco_evaluator\n","\n","model_retina_res.to(device)\n","model_faster_res.to(device)\n","model_faster_mo.to(device)\n","\n","print(my_eval(model_retina_res, data_loader_test, device=device))\n","print(my_eval(model_faster_res, data_loader_test, device=device))\n","print(my_eval(model_faster_mo, data_loader_test, device=device))"]},{"cell_type":"code","execution_count":null,"id":"09be6df3","metadata":{"id":"09be6df3"},"outputs":[],"source":["dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=True))\n","dataset_test2 = PennFudanDataset('PennFudanPed', get_transform(train=True))\n","dataset_test3 = PennFudanDataset('PennFudanPed', get_transform(train=True))\n","\n","dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n","dataset_test2 = torch.utils.data.Subset(dataset_test2, indices[-50:])\n","dataset_test3 = torch.utils.data.Subset(dataset_test3, indices[-50:])\n","\n","# define training and validation data loaders\n","\n","data_loader_test2 = torch.utils.data.DataLoader(\n","    dataset_test2, batch_size=1, shuffle=False, pin_memory=True,\n","    collate_fn=utils.collate_fn)\n","data_loader_test3 = torch.utils.data.DataLoader(\n","    dataset_test3, batch_size=1, shuffle=False, pin_memory=True,\n","    collate_fn=utils.collate_fn)\n","\n","data_loader_test = torch.utils.data.DataLoader(\n","    dataset_test, batch_size=1, shuffle=False, pin_memory=True,\n","    collate_fn=utils.collate_fn)"]},{"cell_type":"markdown","id":"4a20a2f1","metadata":{"id":"4a20a2f1"},"source":["### Load the models which trained on original data"]},{"cell_type":"code","execution_count":null,"id":"847ec09a","metadata":{"id":"847ec09a"},"outputs":[],"source":["# load models\n","model_retina_res = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=False)\n","model_retina_res.load_state_dict(torch.load('models/retina_res.pth'))\n","\n","model_faster_res = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n","model_faster_res.load_state_dict(torch.load('models/faster_res.pth'))\n","\n","model_faster_mo = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=False)\n","model_faster_mo.load_state_dict(torch.load('models/faster_mo.pth'))"]},{"cell_type":"markdown","id":"9be04afa","metadata":{"id":"9be04afa"},"source":["### Test- Time augmentation experiments\n","#### With models trained on original dataset and test on colouring transformed dataset, then use ensenble voting to decide the predictions"]},{"cell_type":"code","execution_count":null,"id":"126e4ba8","metadata":{"id":"126e4ba8"},"outputs":[],"source":["from torchmetrics.detection.mean_ap import MeanAveragePrecision\n","from pprint import pprint\n","import math\n","metric = MeanAveragePrecision()\n","\n","model_faster_res.to(device).eval()\n","model_retina_res.to(device).eval()\n","model_faster_mo.to(device).eval()\n","\n","i = 0\n","for imgs in zip(data_loader_test, data_loader_test2, data_loader_test3):\n","    print(i)\n","    \n","    (img1, tar1) , (img2, tar2), (img3, tar3) = imgs[0], imgs[1], imgs[2]\n","\n","    with torch.no_grad():\n","        img1 = img1[0].to(device)\n","        tar1 = tar1[0]\n","        img2 = img2[0].to(device)\n","        tar2 = tar2[0]\n","        img3 = img3[0].to(device)\n","        tar3 = tar3[0]\n","        pred1_1 = model_retina_res([img1])\n","        pred1_2 = model_faster_res([img1])\n","        pred1_3 = model_faster_mo([img1])\n","        \n","        pred2_1 = model_retina_res([img2])\n","        pred2_2 = model_faster_res([img2])\n","        pred2_3 = model_faster_mo([img2])\n","        \n","        pred3_1 = model_retina_res([img3])\n","        pred3_2 = model_faster_res([img3])\n","        pred3_3 = model_faster_mo([img3])\n","        \n","        box1_1 = pred1_1[0]['boxes'].cpu().numpy().tolist()\n","        box1_2 = pred1_2[0]['boxes'].cpu().numpy().tolist()\n","        box1_3 = pred1_3[0]['boxes'].cpu().numpy().tolist()\n","        \n","        box2_1 = pred2_1[0]['boxes'].cpu().numpy().tolist()\n","        box2_2 = pred2_2[0]['boxes'].cpu().numpy().tolist()\n","        box2_3 = pred2_3[0]['boxes'].cpu().numpy().tolist()\n","        \n","        box3_1 = pred3_1[0]['boxes'].cpu().numpy().tolist()\n","        box3_2 = pred3_2[0]['boxes'].cpu().numpy().tolist()\n","        box3_3 = pred3_3[0]['boxes'].cpu().numpy().tolist()\n","        boxes = box1_1+box1_2+box1_3+box2_1+box2_2+box2_3+box3_1+box3_2+box3_3\n","        #print(boxes)\n","        #boxes = np.concatenate((box1_1, box1_2, box1_3, box2_1, box2_2, box2_3, box3_1, box3_2, box3_3), axis=0)\n","        #print(boxes)\n","        score1_1 = pred1_1[0]['scores'].cpu().numpy()\n","        score1_2 = pred1_2[0]['scores'].cpu().numpy()\n","        score1_3 = pred1_3[0]['scores'].cpu().numpy()\n","        \n","        score2_1 = pred2_1[0]['scores'].cpu().numpy()\n","        score2_2 = pred2_2[0]['scores'].cpu().numpy()\n","        score2_3 = pred2_3[0]['scores'].cpu().numpy()\n","        \n","        score3_1 = pred3_1[0]['scores'].cpu().numpy()\n","        score3_2 = pred3_2[0]['scores'].cpu().numpy()\n","        score3_3 = pred3_3[0]['scores'].cpu().numpy()\n","        \n","        scores = np.concatenate((score1_1, score1_2, score1_3, score2_1, score2_2, score2_3, score3_1, score3_2, score3_3), 0)\n","        #print(scores)\n","        boxx = boxes.copy()\n","        pick = ensemble(boxes, option='unanimous')\n","        #print(len(pick))\n","        idx = []\n","        boxes = boxx\n","        for j in range(len(pick)):\n","            for k in range(len(boxes)):\n","                if (pick[j] == boxes[k]):#.all():\n","                    idx.append(k)\n","                    break\n","        #idx = np.array(idx)\n","        #print(idx)\n","        if len(idx)==0:\n","            continue\n","        pick = torch.from_numpy(np.array(pick))\n","\n","        img = torchvision.transforms.ConvertImageDtype(torch.uint8)(img1.cpu())\n","        result = torchvision.utils.draw_bounding_boxes(img, pick)\n","        out = Image.fromarray(result.permute(1, 2, 0).contiguous().numpy())\n","\n","       # out.save('imgs/test_')\n","        out.show()\n","        \n","        prediction = [{\n","            'boxes' : pick.to(device),\n","            'scores' : torch.from_numpy(scores[idx]).to(device),\n","            'labels' : torch.ones(len(pick)).to(device),\n","        }]\n","        target = [{\n","            'boxes' : tar1['boxes'].to(device),\n","            'labels' : tar1['labels'].to(device),\n","        }]\n","\n","    metric.update(prediction, target)\n","    pprint(metric.compute())\n","    i += 1\n","        \n","        \n","    "]}],"metadata":{"colab":{"collapsed_sections":[],"name":"AUG_TTA_experiments.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"nbformat":4,"nbformat_minor":5}