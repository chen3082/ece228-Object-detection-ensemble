{"cells":[{"cell_type":"code","execution_count":null,"id":"f71821eb","metadata":{"id":"f71821eb"},"outputs":[],"source":["#!unzip PennFudanPed.zip"]},{"cell_type":"code","execution_count":null,"id":"f2cd93dd","metadata":{"id":"f2cd93dd"},"outputs":[],"source":["import os\n","import numpy as np\n","import torch\n","import torch.utils.data\n","from PIL import Image\n","from torch.optim.lr_scheduler import StepLR\n","import timm\n","import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection import RetinaNet\n","from engine_my import train_one_epoch, evaluate\n","import utils\n","import torchvision.transforms as T\n","\n","class PennFudanDataset(torch.utils.data.Dataset):\n","    def __init__(self, root, transforms=None):\n","        self.root = root\n","        self.transforms = transforms\n","        # load all image files, sorting them to\n","        # ensure that they are aligned\n","        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n","        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n","\n","    def __getitem__(self, idx):\n","        # load images ad masks\n","        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n","        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n","        img = Image.open(img_path).convert(\"RGB\")\n","        # note that we haven't converted the mask to RGB,\n","        # because each color corresponds to a different instance\n","        # with 0 being background\n","        mask = Image.open(mask_path)\n","\n","        mask = np.array(mask)\n","        # instances are encoded as different colors\n","        obj_ids = np.unique(mask)\n","        # first id is the background, so remove it\n","        obj_ids = obj_ids[1:]\n","\n","        # split the color-encoded mask into a set\n","        # of binary masks\n","        masks = mask == obj_ids[:, None, None]\n","\n","        # get bounding box coordinates for each mask\n","        num_objs = len(obj_ids)\n","        boxes = []\n","        for i in range(num_objs):\n","            pos = np.where(masks[i])\n","            xmin = np.min(pos[1])\n","            xmax = np.max(pos[1])\n","            ymin = np.min(pos[0])\n","            ymax = np.max(pos[0])\n","            boxes.append([xmin, ymin, xmax, ymax])\n","\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        # there is only one class\n","        labels = torch.ones((num_objs,), dtype=torch.int64)\n","        masks = torch.as_tensor(masks, dtype=torch.uint8)\n","\n","        image_id = torch.tensor([idx])\n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","        # suppose all instances are not crowd\n","        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n","\n","        target = {}\n","        target[\"boxes\"] = boxes\n","        target[\"labels\"] = labels\n","        target[\"masks\"] = masks\n","        target[\"image_id\"] = image_id\n","        target[\"area\"] = area\n","        target[\"iscrowd\"] = iscrowd\n","\n","        if self.transforms is not None:\n","            img = self.transforms(img)\n","\n","        return img, target\n","\n","    def __len__(self):\n","        return len(self.imgs)\n","\n","\n","def get_transform(train):\n","    transforms = []\n","    # converts the image, a PIL image, into a PyTorch Tensor\n","    transforms.append(T.ToTensor())\n","    if train:\n","        # during testing, randomly colour imgs for data augmentation\n","        #transforms.append(T.ColorJitter(hue=(-0.4, 0.4)))\n","        \n","        # Blurs image with randomly chosen Gaussian blur.\n","        #transforms.append(T.GaussianBlur(7, sigma=(0.1, 2.0)))\n","        \n","        # Inverts the colors of the given image randomly with a given probability.\n","        #transforms.append(T.RandomInvert(p=0.5))\n","        \n","        # Solarize the image randomly with a given probability by inverting all pixel values above a threshold.\n","        transforms.append(T.RandomSolarize(0.6, p=0.5))\n","        \n","        # Adjust the sharpness of the image randomly with a given probability. 0: blur, 2: sharp\n","        #transforms.append(T.RandomAdjustSharpness(5, p=1))\n","\n","        # Autocontrast the pixels of the given image randomly with a given probability.b\n","        #transforms.append(T.RandomAutocontrast(p=0.))\n","    return T.Compose(transforms)\n","\n","# use our dataset and defined transformations\n","dataset = PennFudanDataset('PennFudanPed', get_transform(train=False))\n","dataset2 = PennFudanDataset('PennFudanPed', get_transform(train=True))\n","dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n","\n","# split the dataset in train and test set\n","torch.manual_seed(1)\n","indices = torch.randperm(len(dataset)).tolist()\n","dataset = torch.utils.data.Subset(dataset, indices[:-50])\n","dataset2 = torch.utils.data.Subset(dataset2, indices[:-50])\n","dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n","\n","\n","# define training and validation data loaders\n","data_loader = torch.utils.data.DataLoader(\n","    dataset, batch_size=2, shuffle=True, pin_memory=True,\n","    collate_fn=utils.collate_fn)\n","\n","data_loader2 = torch.utils.data.DataLoader(\n","    dataset2, batch_size=2, shuffle=True, pin_memory=True,\n","    collate_fn=utils.collate_fn)\n","\n","data_loader_test = torch.utils.data.DataLoader(\n","    dataset_test, batch_size=1, shuffle=False, pin_memory=True,\n","    collate_fn=utils.collate_fn)\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","# our dataset has two classes only - background and person\n","num_classes = 2\n","\n","\n","img, _ = dataset2[0]\n","img = torchvision.transforms.ConvertImageDtype(torch.uint8)(img.cpu())\n","out = Image.fromarray(img.permute(1, 2, 0).contiguous().numpy())\n","out.show()"]},{"cell_type":"markdown","id":"80fc65d7","metadata":{"id":"80fc65d7"},"source":["### retinanet with resnet50 backbone"]},{"cell_type":"code","execution_count":null,"id":"69d5b77d","metadata":{"id":"69d5b77d","scrolled":true},"outputs":[],"source":["model_retina_res = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True)\n","# move model to the right device\n","model_retina_res.to(device)\n","\n","# construct an optimizer\n","params = [p for p in model_retina_res.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(params, lr=0.005,\n","                            momentum=0.9, weight_decay=0.0005)\n","\n","# and a learning rate scheduler which decreases the learning rate by\n","# 10x every 3 epochs\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n","                                               step_size=3,\n","                                               gamma=0.1)\n","\n","for epoch in range(10):\n","    # train for one epoch, printing every 10 iterations\n","    train_one_epoch(model_retina_res, optimizer, data_loader, device, epoch, print_freq=10)\n","    train_one_epoch(model_retina_res, optimizer, data_loader2, device, epoch, print_freq=10)\n","    # update the learning rate\n","    lr_scheduler.step()\n","    # evaluate on the test dataset\n","    #evaluate(model_retina_res, data_loader_test, device=device)\n","    \n","\n","evaluate(model_retina_res, data_loader_test, device=device)\n"]},{"cell_type":"markdown","id":"76d12138","metadata":{"id":"76d12138"},"source":["### Faser rcnn with resnet50 backbone"]},{"cell_type":"code","execution_count":null,"id":"9e44dab7","metadata":{"id":"9e44dab7"},"outputs":[],"source":["#wandb.init(project='228_project', name=\"faster_res\")\n","model_faster_res = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","model_faster_res.to(device)\n","\n","# construct an optimizer\n","params = [p for p in model_faster_res.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(params, lr=0.005,\n","                            momentum=0.9, weight_decay=0.0005)\n","\n","# and a learning rate scheduler which decreases the learning rate by\n","# 10x every 3 epochs\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n","                                               step_size=3,\n","                                               gamma=0.1)\n","\n","for epoch in range(10):\n","    # train for one epoch, printing every 10 iterations\n","    train_one_epoch(model_faster_res, optimizer, data_loader, device, epoch, print_freq=10)\n","    train_one_epoch(model_retina_res, optimizer, data_loader2, device, epoch, print_freq=10)\n","    # update the learning rate\n","    lr_scheduler.step()\n","    # evaluate on the test dataset\n","\n","evaluate(model_faster_res, data_loader_test, device=device)\n","\n"]},{"cell_type":"markdown","id":"3e0b227d","metadata":{"id":"3e0b227d"},"source":["### Faser rcnn with mobilenet backbone"]},{"cell_type":"code","execution_count":null,"id":"aeafc01f","metadata":{"id":"aeafc01f"},"outputs":[],"source":["#wandb.init(project='228_project', name=\"faster_mo\")\n","model_faster_mo = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n","model_faster_mo.to(device)\n","\n","# construct an optimizer\n","params = [p for p in model_faster_mo.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(params, lr=0.005,\n","                            momentum=0.9, weight_decay=0.0005)\n","\n","# and a learning rate scheduler which decreases the learning rate by\n","# 10x every 3 epochs\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n","                                               step_size=3,\n","                                               gamma=0.1)\n","\n","\n","for epoch in range(10):\n","    # train for one epoch, printing every 10 iterations\n","    train_one_epoch(model_faster_mo, optimizer, data_loader, device, epoch, print_freq=10)\n","    train_one_epoch(model_retina_res, optimizer, data_loader2, device, epoch, print_freq=10)\n","    \n","    # update the learning rate\n","    lr_scheduler.step()\n","    # evaluate on the test dataset\n","evaluate(model_faster_mo, data_loader_test, device=device)"]},{"cell_type":"code","execution_count":null,"id":"a790f460","metadata":{"id":"a790f460"},"outputs":[],"source":["torch.save(model_retina_res.state_dict(), 'models/retina_res_aug.pth')\n","torch.save(model_faster_res.state_dict(), 'models/faster_res_aug.pth')\n","torch.save(model_faster_mo.state_dict(), 'models/faster_mo_aug.pth')"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"augment_exp.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"nbformat":4,"nbformat_minor":5}